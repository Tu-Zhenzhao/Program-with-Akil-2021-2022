\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{xcolor}


\newcommand{\an}[1]{{\leavevmode\color{blue}{#1}}}
\newcommand{\ex}[1]{{\leavevmode\color{red}{#1}}}

\title{Program Study Note}
\author{Zhenzhao Tu}
\date{Nov 30 2021}

\begin{document}

\maketitle

\section{The spectral theorem for symmetric matrices}
Suppose $A \in$ 
${\rm I\!R}^{n \times n}$ is symmetric. Then
\\
\begin{enumerate}
	\item every eigenvalue $\lambda$ of $A$ is a real number and there exists a (real) eigenvector ${u \in \rm I\!R}^{n}$ corresponding to $\lambda$:  $Au=\lambda u$;
	\item eigenvectors corresponding to distinct eigenvalues are necessarily orthogonal: 
	\[Au^{(1)} = \lambda_1 u^{(1)} , Au^{(2)} = \lambda_2 u^{(2)}, \lambda_1 \neq \lambda_2, u^{(1)}u^{(2)} = 0.\]
	\item there exists a diagonal matrix  $D\in{\bf {\rm R}}^{n\times n}$ and an orthogonal matrix  $U\in{\bf {\rm R}}^{n\times n}$ such that $A=UDU^T$. The diagonal entries of $D$ are the eigenvalues of $A$ and the columns of $U$ are the corresponding eigenvectors: 
	\[D = diag(\lambda_1, \lambda_2, \lambda_3, ..., \lambda_n),\]
	\[U = [u^{(1)} | u^{(1)} | u^{(2)} | ... | u^{(n)}],\]
	\[Au^{(n)} = \lambda_n u^{(n)}, i = 1,2,3,...,n.\]
\end{enumerate}
An orthogonal matrix $U$ satisfies, by definition, $U^T=U^{-1}$, which means that the columns of $U$ are orthonormal (that is, any two of them are orthogonal and each has norm one). The expression $A=UDU^T$ of a symmetric matrix in terms of its eigenvalues and eigenvectors is referred to as the \textbf{spectral decomposition} of $A$. 
\\
The spectral theorem implies that there is a change of variables which transforms A into a diagonal matrix.

\an{Don't go into the proof yet. First state the SVD. Second, discuss some of its consequences at a high level. Third, prove it.}
\section{Singular Value Decomposition(SVD)}

\subsection{Definition of SVD(Singular Value Decomposition)}

First set :
$A \in$
${\rm I\!R}^{m \times n}$, assume $n \leq m$,
$r = rank(A) \leq n$
\\
Then : 
$\exists$
$U \in $
${\rm I\!R}^{m \times r}$,
$V \in$
${\rm I\!R}^{r \times n}$
(Here $U$ and $V$ are both have orthogonal columns), and
$\exists$
$\Sigma$
$\in$
${\rm I\!R}^{r \times r}$.
\\
That means
$\Sigma$
has to be a \textbf{diagonal} matrix with strictly positive entries such that:

\begin{equation}
	A = U \Sigma V^T
\end{equation}
\\
Which express by:
\begin{equation}
	A = \sum_{j=1}^{r} \ \sigma_j u_j v_j^T
\end{equation}
where:
\begin{itemize}
	\item $U$ is an $m \times m$ orthogonal matrix.
	\item $V$ is an $n \times n$ orthogonal matrix.
	\item $\Sigma$ is an $m \times n$ matrix whose $i^{th}$ diagonal entry equals the $i^{th}$ singular value $\sigma_i$ for $i$ = 1,...,r. All other entries of $\Sigma$ are zero.
\end{itemize}

\subsection{Find a SVD}
Let $A$ be an 
$m \times n$
matrix with 
$\sigma_1 \geq \sigma_2 \geq ... \geq \sigma_1 \geq 0$
, and let
$r$
denote the number of nonzero singular values
$A$.
\vspace{1mm}

Let 
$v_1,...,v_n$
be an orthogonal basis of
${\rm I\!R}^{m \times n}$,
where
$v_i$
is an eigenvector of 
$A^TA$
with eigenvalue
$\sigma^2_i$.
\\~\\
\textbf{Theorem 2.1.1}
\textit{Let $A$ be $m \times n$ matrix. Then $A$ has a (not unique)singular value decomposition $A = U \Sigma V^T$, where  $U$ and $V$ are as follows:}
\begin{itemize}
	\item The columns of $V$ are orthogonal eigenvectors $v_1, ... , v_n$ of $A^TA$ where $A^TAv_i= \sigma_i^2 v_i$.
	\item If $i \geq r$, so that $\sigma_i \not= 0$, then the $i^th$ column of $U$ is $\sigma_i^{-1} Av_i $. these columns are orthogonal, and the remaining columns of $U$ are obtained by arbitrarily extending to an orthogonal basis for ${\rm I\!R}^{m}.$
\end{itemize}
\textbf{Theorem 2.1.1} Find a SVD of 

\[A = 
	\begin{pmatrix}
		4 & 11 & 14 \\
		8 & 7 & -2 
	\end{pmatrix}\]
\vspace{1mm} 

\textit{Step 1.} Since $A$ is not a symmetric matrix, it cannot  have eigenvalue, we need to find the eigenvalue of $A^TA$. By computing in python, we can get $A^TA$ easily

\[A^TA= 
	\begin{pmatrix}
		80 & 100 & 40 \\
		100 & 170 & 140 \\
		40 & 140 & 200 
	\end{pmatrix}.\]
\\
We can compute three eigenvalues of $A^TA$: $\lambda_1 = 360, \lambda_2 = 90, \lambda_3 = 0$. By definition of singular values, we can find $\sigma_1 = \sqrt{360}, \sigma_2 = \sqrt{90}, \sigma_3 = \sqrt{0}$. By definition of $\Sigma$ in SVD, it has to be a $2 \times 3$ matrix

\[\Sigma= 
\begin{pmatrix}
	\sqrt{360} & 0 & 0 \\
	0 & \sqrt{90} & 0 \\
\end{pmatrix}.\]

\textit{Step 2.} After found singular value, we need to find matrix $V$. We know that $V$ is consist of orthogonal basis of eigenvectors of $A^TA$

\[v_1 = 
	\begin{pmatrix}
		-1/3 \\
		-2/3 \\
		-2/3
	\end{pmatrix},
	v_2 = \begin{pmatrix}
				2/3 \\
				-1/3 \\
				2/3
			\end{pmatrix},
			v_3 = \begin{pmatrix}
							2/3 \\
							-2/3 \\
							1/3
						\end{pmatrix}\]
\\
These three vectors are columns of $V$

\[V= 
\begin{pmatrix}
	-1/3 & -2/3 & 2/3 \\
	-2/3 & -1/3 & -2/3 \\
	-2/3 & 2/3 & 1/3 
\end{pmatrix}.\]

\textit{Step 2.} Now, it is time to find the last one matrix $U$. There are two ways to find $U$. The first one is using definition $u_j = \frac{Av_j}{\sigma_j} $ for every $j$ in $1,...,n$. From this equation, we can simply write down

\[\sigma_1^{-1} Av_1 = \frac{1}{\sqrt{360}} 
	\begin{pmatrix}
		18 \\
		6
	\end{pmatrix},
	\sigma_2^{-1} Av_2 = \frac{1}{\sqrt{90}} 
	\begin{pmatrix}
		3 \\
		9
	\end{pmatrix}\]
Since $u_1$ and $u-2$ are columns of $U$ and $U$ is a strictly $2 \times 2$ matrix, we can write $U$ as 

\[U= 
\begin{pmatrix}
	\frac{3}{\sqrt{10}} & \frac{1}{\sqrt{10}} \\
	\frac{1}{\sqrt{10}} & \frac{-3}{\sqrt{10}}  
\end{pmatrix}.\]

\vspace{1mm} 

In conclusion, we now have singular value decomposition

\[A= 
\begin{pmatrix}
	\frac{3}{\sqrt{10}} & \frac{1}{\sqrt{10}} \\
	\frac{1}{\sqrt{10}} & \frac{-3}{\sqrt{10}}  
\end{pmatrix}
\begin{pmatrix}
	\sqrt{360} & 0 & 0 \\
	0 & \sqrt{90} & 0 \\
\end{pmatrix}
\begin{pmatrix}
	-1/3 & -2/3 & 2/3 \\
	-2/3 & -1/3 & -2/3 \\
	-2/3 & 2/3 & 1/3 
\end{pmatrix}^T.\]

 \subsection{Proof}
At first, let's set:
\begin{equation}
	K = A^T A (\geq 0)
\end{equation}
\\
Then, $K$ can be multiply by $x^T$ and $x$:
\begin{equation}
	x^T K x
\end{equation}
\\
By doing a small calculation, we get:
\begin{equation}
	x^T A^T A x = \left\langle Ax, Ax \right\rangle
\end{equation}
\\
From equation (5), we can simply know length of $Ax$  are equal and  greater than $0$:
\begin{equation}\label{eq:Ax-norm}
	||Ax||_2^2 \geq 0
\end{equation}

By using \textbf{Spectral Theorem}: 
\begin{equation}
	K = V \Lambda V^T \in {\rm I\!R}^{n \times n}
\end{equation}
\\
The $\Lambda$ in $K$ is diagonal matrix, where $V$ is an orthonormal matrix whose columns are the eigenvectors of $A^TA$ and where $r \leq n$ and $r = rank(A) = rank(A^TA) $. Now we define a quantity $\sigma_i$ (the \textit{singular value}) such this equation with $\lambda_i$:
Now let's define an equation:
\begin{equation}
	\sigma_j^2 = \lambda_j     \quad   (\sigma_j = \sqrt{\lambda_j} \geq 0)
\end{equation}
 Assuming there are $\lambda_1$ to $\lambda_n$, and each $\lambda$ is greater than or equal to $0$. The order of $\lambda_j$: $\lambda_j \geq \lambda_{j+1}$, 
$\forall$
$j = 1, ... ,(n-1)$.
For the $i$-th eigenvector-eigenvalue pair, we have
\[A^TAv_{i} = (\sigma_i)^2v_i.\]
It is easy to know that  $v_i$ is the eigenvector of $A^TA$.
For now, assume that we have a full-rank matrix ($\sigma_i \geq 0$ for all $i$). Define a new matrix $U$ is an orthonormal matrixr whose columns $u_i$ such that
\begin{equation}
	u_j = \frac{A v_j}{\sigma_j} \qquad u_j \in {\rm I\!R}^{m} \mbox{and}  v_j \in {\rm I\!R}^{n}
\end{equation}
\\
Since $u_i$ and $v_i$ are orthonormal columns of $U$ and $V$,
$u_j$ and $v_j$ in these two equation satisfied
 $\left\langle u_j, u_k \right\rangle = \delta_{j,k}$ 
 and 
 $\left\langle v_j, v_k \right\rangle = \delta_{j,k}$, 
 the $\delta$ in here is such this equation:
\begin{equation}
		\delta_{j,k}=\left\{
		\begin{array}{ll}
			1, & \mbox{$j=k$}.\\
			0, & \mbox{$j \neq k$}.
		\end{array}
		\right.
\end{equation}
\\
We can easily know that $K$ is a symmetric matrix, so we can write down it eigenvalue and eigenvector form:
\begin{equation}
	Kv_j = \lambda_j v_j
\end{equation}  
\\
Now, if we multiply $A$ in both sides, we can prove a result below:
\begin{align*}
	AKv_j  &= \lambda_j A v_j \\
	(AA^T)Av_j  &= \lambda_j A v_j \\
\end{align*}
\\
both sides divided by $\sigma_j$
\begin{align*}
	(AA^T)u_j  &= \lambda_j  u_j
\end{align*}
\\
From the last equation, we know two things: $(1)$ $u_j$ is an eigenvector of $AA^T$; $(2)$ $\lambda_j$ is an eigenvalue of $AA^T$ .
\\
Start at $Av_j$, we can re-write it as:
\begin{equation}
	Av_j = \sigma_j u _j
\end{equation}
To make our proof easier, here we define $j = 1 ... r$ $(assume \, \textbf{r=n})$.
\\
Don't forget $U$ and $V$ have orthogonal columns, they can be written in this form:
\begin{equation}
	AV = U \Sigma
\end{equation}
\\
Since $V$ is a $n \times n$ matrix with orthogonal columns, it is an orthogonal  matrix. That means $V^T = V^{-1}$.
\\
When we multiply $V^{-1}$ in two sides, we can get our final answer:
\begin{equation}
	A = U \Sigma V^T 
\end{equation} 

\newpage
\section{The SVD for Derivatives and Integrals}
I add this section is because it will help me to understand SVD in a new way. According to textbook,  this is the clearest example of the SVD. At first we can think $A$ as an \textit{operator} instead of matrix. Now we can write down integral and derivative of function in terms of A and operator D:
\[ Integral: Ax(s) = \int_{0}^{s} x(t) \,dt\] 
\[Derivative: Dx(t) = \frac{dx}{dt}.\]
By the Fundamental Theorem of Calculus, $D$ is the inverse of $A$. More exactly $D$ is a left inverse of $A$. Derivative of integral equals original function, so $DA = I$. We call this as \textbf{\textit{$D$ is the pseudoinverse of $A$}}.




\end{document}
