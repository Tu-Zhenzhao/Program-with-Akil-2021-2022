\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}

\title{Program Study Note}
\author{Zhenzhao Tu}
\date{Nov 30 2021}

\begin{document}

\maketitle

\section{Proof of SVD}

\subsection{Theorem of SVD(Singular Value Decomposition)}
First set :
$A \in$
${\rm I\!R}^{m \times n}$,
$r = rank(A) \leq n$
\\
Then : 
$\exists$
$U \in $
${\rm I\!R}^{m \times r}$,
$V \in$
${\rm I\!R}^{r \times n}$
(Here $U$ and $V$ are both have orthogonal columns), and
$\exists$
$\Sigma$
$\in$
${\rm I\!R}^{r \times r}$.
\\
That means
$\Sigma$
has to be a \textbf{diagonal} matrix with strictly positive entries such that:

\begin{equation}
	A = U \Sigma V^T
\end{equation}
\\
Which express by:
\begin{equation}
	A = \sum_{j=1}^{r} \ \sigma_j u_j v_j^T
\end{equation}

\subsection{Proof}
At first, let's set:
\begin{equation}
	K = A^T A (\geq 0)
\end{equation}
\\
Then, $K$ can be multiply by $x^T$ and $x$:
\begin{equation}
	x^T K x
\end{equation}
\\
By doing a small calculation, we get:
\begin{equation}
	x^T A^T A x = <Ax, Ax>
\end{equation}
\\
From equation 5, we can simply know length of $Ax$  are equal and  greater than $0$:
\begin{equation}
	||Ax||_2^2 \geq 0
\end{equation}
\\
By using \textbf{Spectral Theorem}:
\begin{equation}
	K = V \Lambda V^T \in {\rm I\!R}^{n \times n}
\end{equation}
\\
The $\Lambda$ in $K$ is diagonal matrix. Assuming there are $\lambda_1$ to $\lambda_n$, and each $\lambda$ is greater than or equal to $0$. In other case, 
$\lambda_j$ 
$\geq 0$.
The order of $\lambda_j$: $\lambda_j \geq \lambda_{j+1}$, 
$\forall$
$j = 1, ... ,(n-1)$.
\\
Now let's define an equation:
\begin{equation}
	\sigma_j^2 = \lambda_j     \quad   (\sigma_j = \sqrt{\lambda_j} \geq 0)
\end{equation}
\\
and
\\
\begin{equation}
	u_j = \frac{A v_j}{\sigma_j} \qquad u_j \in {\rm I\!R}^{m} \mbox{and}  v_j \in {\rm I\!R}^{n}
\end{equation}
\\
$u_j$ and $v_j$ in these two equation satisfied
 $<u_j, u_k> = \delta_{j,k}$ 
 and 
 $<v_j, v_k> = \delta_{j,k}$,
 the $\delta$ in here is such this equation:
\begin{equation}
		\delta_{j,k}=\left\{
		\begin{array}{ll}
			1, & \mbox{$j=k$}.\\
			0, & \mbox{$j \neq k$}.
		\end{array}
		\right.
\end{equation}
\\
We can easily know that $K$ is a symmetric matrix, so we can write down it eigenvalue and eigenvector form:
\begin{equation}
	Kv_j = \lambda_j v_j
\end{equation}  
\\
Now, if we multiply $A$ in both sides, we can prove a result below:
\begin{align*}
	AKv_j  &= \lambda_j A v_j \\
	(AA^T)Av_j  &= \lambda_j A v_j \\
\end{align*}
\\
both sides divided by $\sigma_j$
\begin{align*}
	(AA^T)u_j  &= \lambda_j  u_j
\end{align*}
\\
From the last equation, we know two things: $(1)$ $u_j$ is an eigenvector of $AA^T$; $(2)$ $\lambda_j$ is an eigenvalue of $AA^T$ .
\\[2in]
Back to the matrix that we defined $Av_j$. Start at $Av_j$, we can define it as:
\begin{equation}
	Av_j = \sigma_j u _j
\end{equation}
To make our proof easier, here we define $j = 1 ... r$ $(assume \, \textbf{r=n})$.
\\[3in]
Don't forget $U$ and $V$ have orthogonal columns, they can be written in this form:
\begin{equation}
	AV = U \Sigma
\end{equation}
\\
Since $V$ is a $n \dots n$ matrix with orthogonal columns, it is an orthogonal  matrix. That means $V^T = V^{-1}$.
\\
When we multiply $V^{-1}$ in two sides, we can get our final answer:
\begin{equation}
	A = U \Sigma V^T 
\end{equation} 



\begin{figure}
	%\includegraphics[width=1.0\textwidth]{images/picture.png}
	\caption{Here is an example of a Figure, with an embedded picture.}
\end{figure}









\end{document}