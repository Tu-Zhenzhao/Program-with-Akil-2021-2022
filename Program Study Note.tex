\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{hyperref}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    pdftitle={Overleaf Example},
    pdfpagemode=FullScreen,
    }
    
\urlstyle{same}

\newcommand{\an}[1]{{\leavevmode\color{blue}{#1}}}
\newcommand{\ex}[1]{{\leavevmode\color{red}{#1}}}

\title{Program Study Note}
\author{Zhenzhao Tu}
\date{Nov 30 2021}


\begin{document}

\maketitle


\section{The spectral theorem for symmetric matrices}
Suppose $A \in$ 
${\rm I\!R}^{n \times n}$ is symmetric. Then
\\
\begin{enumerate}
	\item every eigenvalue $\lambda$ of $A$ is a real number and there exists a (real) eigenvector ${u \in \rm I\!R}^{n}$ corresponding to $\lambda$:  $Au=\lambda u$;
	\item eigenvectors corresponding to distinct eigenvalues are necessarily orthogonal: 
	\[Au^{(1)} = \lambda_1 u^{(1)} , Au^{(2)} = \lambda_2 u^{(2)}, \lambda_1 \neq \lambda_2, u^{(1)}u^{(2)} = 0.\]
	\item there exists a diagonal matrix  $D\in{\bf {\rm R}}^{n\times n}$ and an orthogonal matrix  $U\in{\bf {\rm R}}^{n\times n}$ such that $A=UDU^T$. The diagonal 
		entries of $D$ are the eigenvalues of $A$ and the columns of $U$ are the corresponding eigenvectors: 
	\[D = diag(\lambda_1, \lambda_2, \lambda_3, ..., \lambda_n),\]
	\[U = [u^{(1)} | u^{(1)} | u^{(2)} | ... | u^{(n)}],\]
	\[Au^{(n)} = \lambda_n u^{(n)}, i = 1,2,3,...,n.\]
\end{enumerate}
An orthogonal matrix $U$ satisfies, by definition, $U^T=U^{-1}$, which means that the columns of $U$ are orthonormal (that is, any two of them are orthogonal and each has norm one). The expression $A=UDU^T$ of a symmetric matrix in terms of its eigenvalues and eigenvectors is referred to as the \textbf{spectral decomposition} of $A$. 
\\
The spectral theorem implies that there is a change of variables which transforms A into a diagonal matrix.

\an{Don't go into the proof yet. First state the SVD. Second, discuss some of its consequences at a high level. Third, prove it.}
\section{Singular Value Decomposition(SVD)}

\subsection{Definition of SVD(Singular Value Decomposition)}

First set :
$A \in$
${\rm I\!R}^{m \times n}$, assume $n \leq m$,
$r = rank(A) \leq n$
\\
Then : 
$\exists$
$U \in $
${\rm I\!R}^{m \times r}$,
$V \in$
${\rm I\!R}^{r \times n}$
(Here $U$ and $V$ are both have orthogonal columns), and
$\exists$
$\Sigma$
$\in$
${\rm I\!R}^{r \times r}$.
\\
That means
$\Sigma$
has to be a \textbf{diagonal} matrix with strictly positive entries such that:

\begin{equation}
	A = U \Sigma V^T
\end{equation}
\\
Which express by:
\begin{equation}
	A = \sum_{j=1}^{r} \ \sigma_j u_j v_j^T
\end{equation}
where:
\begin{itemize}
	\item $U$ is an $m \times m$ orthogonal matrix.
	\item $V$ is an $n \times n$ orthogonal matrix.
	\item $\Sigma$ is an $m \times n$ matrix whose $i^{th}$ diagonal entry equals the $i^{th}$ singular value $\sigma_i$ for $i$ = 1,...,r. All other entries of $\Sigma$ are zero.
\end{itemize}

\subsection{Find a SVD}
Let $A$ be an 
$m \times n$
matrix with 
$\sigma_1 \geq \sigma_2 \geq ... \geq \sigma_1 \geq 0$
, and let
$r$
denote the number of nonzero singular values
$A$.
\vspace{1mm}

Let 
$v_1,...,v_n$
be an orthogonal basis of
${\rm I\!R}^{m \times n}$,
where
$v_i$ is an eigenvector of 
$A^TA$
with eigenvalue
$\sigma^2_i$.
\\~\\
\textbf{Theorem 2.1.1}
\textit{Let $A$ be $m \times n$ matrix. Then $A$ has a (not unique)singular value decomposition $A = U \Sigma V^T$, where  $U$ and $V$ are as follows:}
\begin{itemize}
	\item The columns of $V$ are orthogonal eigenvectors $v_1, ... , v_n$ of $A^TA$ where $A^TAv_i= \sigma_i^2 v_i$.
	\item If $i \geq r$, so that $\sigma_i \not= 0$, then the $i^th$ column of $U$ is $\sigma_i^{-1} Av_i $. these columns are orthogonal, and the remaining columns of $U$ are obtained by arbitrarily extending to an orthogonal basis for ${\rm I\!R}^{m}.$
\end{itemize}
\textbf{Theorem 2.1.1} Find a SVD of 

\[A = 
	\begin{pmatrix}
		4 & 11 & 14 \\
		8 & 7 & -2 
	\end{pmatrix}\]
\vspace{1mm} 

\textit{Step 1.} Since $A$ is not a symmetric matrix, it cannot  have eigenvalue, we need to find the eigenvalue of $A^TA$. By computing in python, we can get $A^TA$ easily

\[A^TA= 
	\begin{pmatrix}
		80 & 100 & 40 \\
		100 & 170 & 140 \\
		40 & 140 & 200 
	\end{pmatrix}.\]
\\
We can compute three eigenvalues of $A^TA$: $\lambda_1 = 360, \lambda_2 = 90, \lambda_3 = 0$. By definition of singular values, we can find $\sigma_1 = \sqrt{360}, \sigma_2 = \sqrt{90}, \sigma_3 = \sqrt{0}$. By definition of $\Sigma$ in SVD, it has to be a $2 \times 3$ matrix

\[\Sigma= 
\begin{pmatrix}
	\sqrt{360} & 0 & 0 \\
	0 & \sqrt{90} & 0 \\
\end{pmatrix}.\]

\textit{Step 2.} After found singular value, we need to find matrix $V$. We know that $V$ is consist of orthogonal basis of eigenvectors of $A^TA$

\[v_1 = 
	\begin{pmatrix}
		-1/3 \\
		-2/3 \\
		-2/3
	\end{pmatrix},
	v_2 = \begin{pmatrix}
				2/3 \\
				-1/3 \\
				2/3
			\end{pmatrix},
			v_3 = \begin{pmatrix}
							2/3 \\
							-2/3 \\
							1/3
						\end{pmatrix}\]
\\
These three vectors are columns of $V$

\[V= 
\begin{pmatrix}
	-1/3 & -2/3 & 2/3 \\
	-2/3 & -1/3 & -2/3 \\
	-2/3 & 2/3 & 1/3 
\end{pmatrix}.\]

\textit{Step 2.} Now, it is time to find the last one matrix $U$. There are two ways to find $U$. The first one is using definition $u_j = \frac{Av_j}{\sigma_j} $ for every $j$ in $1,...,n$. From this equation, we can simply write down

\[\sigma_1^{-1} Av_1 = \frac{1}{\sqrt{360}} 
	\begin{pmatrix}
		18 \\
		6
	\end{pmatrix},
	\sigma_2^{-1} Av_2 = \frac{1}{\sqrt{90}} 
	\begin{pmatrix}
		3 \\
		9
	\end{pmatrix}\]
Since $u_1$ and $u-2$ are columns of $U$ and $U$ is a strictly $2 \times 2$ matrix, we can write $U$ as 

\[U= 
\begin{pmatrix}
	\frac{3}{\sqrt{10}} & \frac{1}{\sqrt{10}} \\
	\frac{1}{\sqrt{10}} & \frac{-3}{\sqrt{10}}  
\end{pmatrix}.\]

\vspace{1mm} 

In conclusion, we now have singular value decomposition

\[A= 
\begin{pmatrix}
	\frac{3}{\sqrt{10}} & \frac{1}{\sqrt{10}} \\
	\frac{1}{\sqrt{10}} & \frac{-3}{\sqrt{10}}  
\end{pmatrix}
\begin{pmatrix}
	\sqrt{360} & 0 & 0 \\
	0 & \sqrt{90} & 0 \\
\end{pmatrix}
\begin{pmatrix}
	-1/3 & -2/3 & 2/3 \\
	-2/3 & -1/3 & -2/3 \\
	-2/3 & 2/3 & 1/3 
\end{pmatrix}^T.\]

 \subsection{Proof}
At first, let's set:
\begin{equation}
	K = A^T A (\geq 0)
\end{equation}
\\
Then, $K$ can be multiply by $x^T$ and $x$:
\begin{equation}
	x^T K x
\end{equation}
\\
By doing a small calculation, we get:
\begin{equation}
	x^T A^T A x = \left\langle Ax, Ax \right\rangle
\end{equation}
\\
From equation (5), we can simply know length of $Ax$  are equal and  greater than $0$:
\begin{equation}\label{eq:Ax-norm}
	||Ax||_2^2 \geq 0
\end{equation}

By using \textbf{Spectral Theorem}: 
\begin{equation}
	K = V \Lambda V^T \in {\rm I\!R}^{n \times n}
\end{equation}
\\
The $\Lambda$ in $K$ is diagonal matrix, where $V$ is an orthonormal matrix whose columns are the eigenvectors of $A^TA$ and where $r \leq n$ and $r = rank(A) = rank(A^TA) $. Now we define a quantity $\sigma_i$ (the \textit{singular value}) such this equation with $\lambda_i$:
Now let's define an equation:
\begin{equation}
	\sigma_j^2 = \lambda_j     \quad   (\sigma_j = \sqrt{\lambda_j} \geq 0)
\end{equation}
 Assuming there are $\lambda_1$ to $\lambda_n$, and each $\lambda$ is greater than or equal to $0$. The order of $\lambda_j$: $\lambda_j \geq \lambda_{j+1}$, 
$\forall$
$j = 1, ... ,(n-1)$.
For the $i$-th eigenvector-eigenvalue pair, we have
\[A^TAv_{i} = (\sigma_i)^2v_i.\]
It is easy to know that  $v_i$ is the eigenvector of $A^TA$.
For now, assume that we have a full-rank matrix ($\sigma_i \geq 0$ for all $i$). Define a new matrix $U$ is an orthonormal matrixr whose columns $u_i$ such that
\begin{equation}
	u_j = \frac{A v_j}{\sigma_j} \qquad u_j \in {\rm I\!R}^{m} \mbox{and}  v_j \in {\rm I\!R}^{n}
\end{equation}
\\
Since $u_i$ and $v_i$ are orthonormal columns of $U$ and $V$,
$u_j$ and $v_j$ in these two equation satisfied
 $\left\langle u_j, u_k \right\rangle = \delta_{j,k}$ 
 and 
 $\left\langle v_j, v_k \right\rangle = \delta_{j,k}$, 
 the $\delta$ in here is such this equation:
\begin{equation}
		\delta_{j,k}=\left\{
		\begin{array}{ll}
			1, & \mbox{$j=k$}.\\
			0, & \mbox{$j \neq k$}.
		\end{array}
		\right.
\end{equation}
\\
We can easily know that $K$ is a symmetric matrix, so we can write down it eigenvalue and eigenvector form:
\begin{equation}
	Kv_j = \lambda_j v_j
\end{equation}  
\\
Now, if we multiply $A$ in both sides, we can prove a result below:
\begin{align*}
	AKv_j  &= \lambda_j A v_j \\
	(AA^T)Av_j  &= \lambda_j A v_j \\
\end{align*}
\\
both sides divided by $\sigma_j$
\begin{align*}
	(AA^T)u_j  &= \lambda_j  u_j
\end{align*}
\\
From the last equation, we know two things: $(1)$ $u_j$ is an eigenvector of $AA^T$; $(2)$ $\lambda_j$ is an eigenvalue of $AA^T$ .
\\
Start at $Av_j$, we can re-write it as:
\begin{equation}
	Av_j = \sigma_j u _j
\end{equation}
To make our proof easier, here we define $j = 1 ... r$ $(assume \, \textbf{r=n})$.
\\
Don't forget $U$ and $V$ have orthogonal columns, they can be written in this form:
\begin{equation}
	AV = U \Sigma
\end{equation}
\\
Since $V$ is a $n \times n$ matrix with orthogonal columns, it is an orthogonal  matrix. That means $V^T = V^{-1}$.
\\
When we multiply $V^{-1}$ in two sides, we can get our final answer:
\begin{equation}
	A = U \Sigma V^T 
\end{equation} 

\newpage
e
\section{The SVD for Derivatives and Integrals}
I add this section is because it will help me to understand SVD in a new way. According to textbook,  this is the clearest example of the SVD. At first we can think $A$ as an \textit{operator} instead of matrix. Now we can write down integral and derivative of function in terms of A and operator D:
\[ Integral: Ax(s) = \int_{0}^{s} x(t) \,dt\] 
\[Derivative: Dx(t) = \frac{dx}{dt}.\]
By the Fundamental Theorem of Calculus, $D$ is the inverse of $A$. More exactly $D$ is a left inverse of $A$. Derivative of integral equals original function, so $DA = I$. We call this as \textbf{\textit{$D$ is the pseudoinverse of $A$}}.
\\
Assuming $u$ is $sin(kt)$ and $v$ is $cos(kt)$ in $Av = \sigma v$, then we have:
\[A(coskt) = \frac{1}{k}(sinkt)\]
\[D(sinkt) = k(coskt)\]
The input and output spaces of $A$ and $D$ are very like ${\rm I\!R}^{n}$ and ${\rm I\!R}^{m}$ in a $m$ by $n$ matrix.

We know the property of the SVD is that the $v$ and $u$ 's are orthogonal. That also working in those function. The inner product of functions $x_1$ and $x_2$ is the integral of $x_1(t)x_2(t)$. They are represented by:
\[v_k^{T}v_j =  \int_{0}^{2 \pi} (coskt)(cosjk) \,dt = 0\]
\[u_k^{T}u_j =  \int_{0}^{2 \pi} (sinkt)(sinjk) \,dt = 0\]

\section{Pricipal Component Analysis}

i\subsection{Steps of PCA}
\begin{enumerate}
								\item Standardize the range of continuous initial variables
								\item Compute the covariance matrix to identify correlations

								\item Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components
								
								\item Create a feature vector to decide which principal components to keep
								\item Recast the data along the principal components axes

\end{enumerate}

\subsection{Idea of PCA}

\subsubsection{Problem} Many datasets have too many features to be able to explore or understand in a reasonable way. Its difficult to even make a reasonable plot for a high-dimensional dataset.

\subsubsection{Idea} In \href{https://en.wikipedia.org/wiki/Principal_component_analysis}{Principal Component Analysis (PCA)}, we find a small number of new features, which are linear combinations of the old features, that 'explain' most of the variance in the data. The principal component directions are the directions in feature space in which the data is the most variable. 

Before we get into the mathematical description of Principal Component Analysis (PCA), we can gain a lot of intuition by taking a look at \href{http://setosa.io/ev/principal-component-analysis/}{this visual overview} by Victor Powell.

\subsubsection{Mathematical description} Let the $p$ features in our dataset be $x = (x_1, x_2, \ldots x_p)$. We define a new feature, the \textit{first principal component score}, by 
$$
z_1 = \phi_{1,1} x_1 + \phi_{2,1} x_2 + \cdots + \phi_{p,1} x_p = \phi_1^t x 
$$
Here, the coefficients $\phi_{j,1}$ are the \textit{loadings} of the $j$-th feature on the first principal component. The vector $\phi_1 = (\phi_{1,1}, \phi_{2,1},\cdots, \phi_{p,1})$ is called the \textit{loadings vector} for the first principal component. 

We want to find the loadings so that $z_1$ has maximal sample variance. 

Let $X$ be the $n\times p$ matrix where $X_{i,j}$ is the $j$-th feature for item $i$ in the dataset. $X$ is just the collection of the data in a matrix. 

\subsubsection{Important} Assume each of the variables has been normalized to have mean zero, *i.e.*, the columns of $X$ should have zero mean. 

A short calculation shows that the sample variance of $z_1$ is then given by 
$$
Var(z_1) = \frac{1}{n} \sum_{i=1}^n \left( \sum_{j=1}^p \phi_{j,1} X_{i,j} \right)^2. 
$$
The variance can be arbitrarily large if the $\phi_{j,1}$ are allowed to be arbitrarily large. We constrain the $\phi_{j,1}$ to satisfy $\sum_{j=1}^p \phi_{j,1}^2 = 1$. In vector notation, this can be written $\| \phi_1 \| = 1$.

Putting this together, the first principal component is defined by $z_1 = \phi_1^t x$ where $\phi_1$ is the solution to the optimization problem 
\begin{align*}
\max_{\phi_1} \quad & \textrm{Var}(z_1) \\
\text{subject to} \quad & \| \phi_1\|^2 = 1. 
\end{align*}
Using linear algebra, it can be shown that $\phi_1$ is exactly the eigenvector corresponding to the largest eigenvalue of the *covariance matrix*, $X^tX$. 

We similarly define the second principal direction to be the linear combination of the features, 
$z_2 = \phi_2^t x$ with the largest variance, subject to the additional constraint that $z_2$ be uncorrelated with $z_1$. This is equivalent to $\phi_1^t \phi_2 = 0$. This corresponds to taking $\phi_2$ to be the eigenvector corresponding to the second largest eigenvalue of $X^tX$. Higher principal directions are defined analogously. 





\end{document}
